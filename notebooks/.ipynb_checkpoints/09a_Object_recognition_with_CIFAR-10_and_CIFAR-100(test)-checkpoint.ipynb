{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 and CIFAR-100 datasets\n",
    "\n",
    "[CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) are a pair of image classification datasets collected by collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. They are labelled subsets of the much larger [80 million tiny images](dataset). They are a common benchmark task for image classification - a list of current accuracy benchmarks for both data sets are maintained by Rodrigo Benenson [here](http://rodrigob.github.io/are_we_there_yet/build/).\n",
    "\n",
    "As the name suggests, CIFAR-10 has images in 10 classes:\n",
    "\n",
    "    airplane\n",
    "    automobile\n",
    "    bird \n",
    "    cat\n",
    "    deer\n",
    "    dog\n",
    "    frog\n",
    "    horse\n",
    "    ship\n",
    "    truck\n",
    "\n",
    "with 6000 images per class for an overall dataset size of 60000. Each image has three (RGB) colour channels and pixel dimension 32×32, corresponding to a total dimension per input image of 3×32×32=3072. For each colour channel the input values have been normalised to the range [0, 1].\n",
    "\n",
    "CIFAR-100 has images of identical dimensions to CIFAR-10 but rather than 10 classes they are instead split across 100 fine-grained classes (and 20 coarser 'superclasses' comprising multiple finer classes):\n",
    "\n",
    "<table style='border: none;'>\n",
    "    <tbody><tr style='font-weight: bold;'>\n",
    "        <td>Superclass</td>\n",
    "        <td>Classes</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>aquatic mammals</td>\n",
    "        <td>beaver, dolphin, otter, seal, whale</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>fish</td>\n",
    "        <td>aquarium fish, flatfish, ray, shark, trout</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>flowers</td>\n",
    "        <td>orchids, poppies, roses, sunflowers, tulips</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>food containers</td>\n",
    "        <td>bottles, bowls, cans, cups, plates</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>fruit and vegetables</td>\n",
    "        <td>apples, mushrooms, oranges, pears, sweet peppers</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>household electrical devices</td>\n",
    "        <td>clock, computer keyboard, lamp, telephone, television</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>household furniture</td>\n",
    "        <td>bed, chair, couch, table, wardrobe</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>insects</td>\n",
    "        <td>bee, beetle, butterfly, caterpillar, cockroach</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>large carnivores</td>\n",
    "        <td>bear, leopard, lion, tiger, wolf</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>large man-made outdoor things</td>\n",
    "        <td>bridge, castle, house, road, skyscraper</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>large natural outdoor scenes</td>\n",
    "        <td>cloud, forest, mountain, plain, sea</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>large omnivores and herbivores</td>\n",
    "        <td>camel, cattle, chimpanzee, elephant, kangaroo</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>medium-sized mammals</td>\n",
    "        <td>fox, porcupine, possum, raccoon, skunk</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>non-insect invertebrates</td>\n",
    "        <td>crab, lobster, snail, spider, worm</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>people</td>\n",
    "        <td>baby, boy, girl, man, woman</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>reptiles</td>\n",
    "        <td>crocodile, dinosaur, lizard, snake, turtle</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>small mammals</td>\n",
    "        <td>hamster, mouse, rabbit, shrew, squirrel</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>trees</td>\n",
    "        <td>maple, oak, palm, pine, willow</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>vehicles 1</td>\n",
    "        <td>bicycle, bus, motorcycle, pickup truck, train</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>vehicles 2</td>\n",
    "        <td>lawn-mower, rocket, streetcar, tank, tractor</td>\n",
    "    </tr>\n",
    "</tbody></table>\n",
    "\n",
    "Each class has 600 examples in it, giving an overall dataset size of 60000 i.e. the same as CIFAR-10.\n",
    "\n",
    "Both CIFAR-10 and CIFAR-100 have standard splits into 50000 training examples and 10000 test examples. For CIFAR-100 as there is an optional Kaggle competition (see below) scored on predictions on the test set, we have used a non-standard assignation of examples to test and training set and only provided the inputs (and not target labels) for the 10000 examples chosen for the test set. \n",
    "\n",
    "For CIFAR-10 the 10000 test set examples have labels provided: to avoid any accidental over-fitting to the test set **you should only use these for the final evaluation of your model(s)**. If you repeatedly evaluate models on the test set during model development it is easy to end up indirectly fitting to the test labels - for those who have not already read it see this [excellent cautionary note from the MLPR notes by Iain Murray](http://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/notes/w2a_train_test_val.html#warning-dont-fool-yourself-or-make-a-fool-of-yourself). \n",
    "\n",
    "For both CIFAR-10 and CIFAR-100, the remaining 50000 non-test examples have been split in to a 40000 example training dataset and a 10000 example validation dataset, each with target labels provided. If you wish to use a more complex cross-fold validation scheme you may want to combine these two portions of the dataset and define your own functions for separating out a validation set.\n",
    "\n",
    "Data provider classes for both CIFAR-10 and CIFAR-100 are available in the `mlp.data_providers` module. Both have similar behaviour to the `MNISTDataProvider` used extensively last semester. A `which_set` argument can be used to specify whether to return a data provided for the training dataset (`which_set='train'`) or validation dataset (`which_set='valid'`).\n",
    "\n",
    "The CIFAR-100 data provider also takes an optional `use_coarse_targets` argument in its constructor. By default this is set to `False` and the targets returned by the data provider correspond to 1-of-K encoded binary vectors for the 100 fine-grained object classes. If `use_coarse_targets=True` then instead the data provider will return 1-of-K encoded binary vector targets for the 20 coarse-grained superclasses associated with each input instead.\n",
    "\n",
    "Both data provider classes provide a `label_map` attribute which is a list of strings which are the class labels corresponding to the integer targets (i.e. prior to conversion to a 1-of-K encoded binary vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the CIFAR-10 and CIFAR-100 data\n",
    "\n",
    "Before using the data provider objects you will need to make sure the data files are accessible to the `mlp` package by existing under the directory specified by the `MLP_DATA_DIR` path.\n",
    "\n",
    "The data is available as compressed NumPy `.npz` files\n",
    "\n",
    "    cifar-10-train.npz           235MB\n",
    "    cifar-10-valid.npz            59MB\n",
    "    cifar-10-test-inputs.npz      59MB\n",
    "    cifar-10-test-targets.npz     10KB\n",
    "    cifar-100-train.npz          235MB\n",
    "    cifar-100-valid.npz           59MB\n",
    "    cifar-100-test-inputs.npz     59MB\n",
    "\n",
    "\n",
    "in the AFS directory `/afs/inf.ed.ac.uk/group/teaching/mlp/data`.\n",
    "\n",
    "If you are working on DICE one option is to redefine your `MLP_DATA_DIR` to directly point to the shared AFS data directory by editing the `env_vars.sh` start up file for your environment. This will avoid using up your DICE quota by storing the data files in your homespace but may involve slower initial loading of the data on initialising the data providers if many people are trying access the same files at once. The environment variable can be redefined by running\n",
    "\n",
    "```\n",
    "gedit ~/miniconda2/envs/mlp/etc/conda/activate.d/env_vars.sh\n",
    "```\n",
    "\n",
    "in a terminal window (assuming you installed `miniconda2` to your home directory), and changing the line\n",
    "\n",
    "```\n",
    "export MLP_DATA_DIR=$HOME/mlpractical/data\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```\n",
    "export MLP_DATA_DIR=\"/afs/inf.ed.ac.uk/group/teaching/mlp/data\"\n",
    "```\n",
    "\n",
    "and then saving and closing the editor. You will need reload the `mlp` environment using `source activate mlp` and restart the Jupyter notebook server in the reloaded environment for the new environment variable definition to be available.\n",
    "\n",
    "For those working on DICE who have sufficient quota remaining or those using there own machine, an alternative option is to copy the data files in to your local `mlp/data` directory (or wherever your `MLP_DATA_DIR` environment variable currently points to if different). \n",
    "\n",
    "\n",
    "Assuming your local `mlpractical` repository is in your home directory you should be able to copy the required files on DICE by running\n",
    "\n",
    "```\n",
    "cp /afs/inf.ed.ac.uk/group/teaching/mlp/data/cifar*.npz ~/mlpractical/data\n",
    "```\n",
    "\n",
    "On a non-DICE machine, you will need to either [set up local access to AFS](http://computing.help.inf.ed.ac.uk/informatics-filesystem), use a remote file transfer client like `scp` or you can alternatively download the files using the iFile web interface [here](https://ifile.inf.ed.ac.uk/?path=%2Fafs%2Finf.ed.ac.uk%2Fgroup%2Fteaching%2Fmlp%2Fdata&goChange=Go) (requires DICE credentials).\n",
    "\n",
    "As some of the files are quite large you may wish to copy only those you are using currently (e.g. only the files for one of the two tasks) to your local filespace to avoid filling up your quota. The `cifar-100-test-inputs.npz` file will only be needed by those intending to enter the associated optional Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example two-layer classifier models\n",
    "\n",
    "Below example code is given for creating instances of the CIFAR-10 and CIFAR-100 data provider objects and using them to train simple two-layer feedforward network models with rectified linear activations in TensorFlow. You may wish to use this code as a starting point for your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mlp.data_providers import CIFAR10DataProvider, CIFAR100DataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_seed = 12345678\n",
    "rng = np.random.RandomState(random_seed)\n",
    "train_data = CIFAR10DataProvider('train', batch_size=50,rng=rng)\n",
    "valid_data = CIFAR10DataProvider('valid', batch_size=50,rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(inputs, input_dim, output_dim, nonlinearity=tf.nn.relu):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [input_dim, output_dim], stddev=2. / (input_dim + output_dim)**0.5), \n",
    "        'weights')\n",
    "    biases = tf.Variable(tf.zeros([output_dim]), 'biases')\n",
    "    outputs = nonlinearity(tf.matmul(inputs, weights) + biases)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1]], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')\n",
    "num_hidden = 500\n",
    "num_bottleneck_layer = 50\n",
    "\n",
    "with tf.name_scope('fc-layer-1'):\n",
    "    hidden_1 = fully_connected_layer(inputs, train_data.inputs.shape[1], num_hidden)\n",
    "with tf.name_scope('fc-layer-2'):\n",
    "    hidden_2= fully_connected_layer(hidden_1, num_hidden, num_hidden)\n",
    "with tf.name_scope('fc-layer-3'):\n",
    "    hidden_3 = fully_connected_layer(hidden_2, num_hidden, num_hidden)\n",
    "#with tf.name_scope('fc-layer-4'):\n",
    "#    hidden_4 = fully_connected_layer(hidden_3, num_hidden, num_bottleneck_layer)\n",
    "#with tf.name_scope('fc-layer-5'):\n",
    "#    hidden_5 = fully_connected_layer(hidden_4, num_hidden, num_hidden)\n",
    "with tf.name_scope('output-layer'):\n",
    "    outputs = fully_connected_layer(hidden_3, num_hidden, train_data.num_classes, tf.identity)\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    error = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(outputs, targets))\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1)), \n",
    "            tf.float32))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(error)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 800\n",
      "End of epoch 01: err(train)=1.98 acc(train)=0.29 time(train)=12.83\n",
      "800 800\n",
      "End of epoch 02: err(train)=1.78 acc(train)=0.38 time(train)=13.00\n",
      "800 800\n",
      "End of epoch 03: err(train)=1.71 acc(train)=0.40 time(train)=13.09\n",
      "800 800\n",
      "End of epoch 04: err(train)=1.66 acc(train)=0.42 time(train)=13.19\n",
      "800 800\n",
      "End of epoch 05: err(train)=1.62 acc(train)=0.43 time(train)=13.08\n",
      "                 err(valid)=1.64 acc(valid)=0.43\n",
      "800 800\n",
      "End of epoch 06: err(train)=1.59 acc(train)=0.44 time(train)=13.01\n",
      "800 800\n",
      "End of epoch 07: err(train)=1.57 acc(train)=0.45 time(train)=13.01\n",
      "800 800\n",
      "End of epoch 08: err(train)=1.54 acc(train)=0.46 time(train)=13.01\n",
      "800 800\n",
      "End of epoch 09: err(train)=1.52 acc(train)=0.47 time(train)=13.04\n",
      "800 800\n",
      "End of epoch 10: err(train)=1.50 acc(train)=0.48 time(train)=13.04\n",
      "                 err(valid)=1.56 acc(valid)=0.45\n",
      "800 800\n",
      "End of epoch 11: err(train)=1.48 acc(train)=0.48 time(train)=13.00\n",
      "800 800\n",
      "End of epoch 12: err(train)=1.47 acc(train)=0.49 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 13: err(train)=1.45 acc(train)=0.49 time(train)=13.01\n",
      "800 800\n",
      "End of epoch 14: err(train)=1.44 acc(train)=0.50 time(train)=13.02\n",
      "800 800\n",
      "End of epoch 15: err(train)=1.42 acc(train)=0.51 time(train)=13.00\n",
      "                 err(valid)=1.50 acc(valid)=0.48\n",
      "800 800\n",
      "End of epoch 16: err(train)=1.41 acc(train)=0.51 time(train)=13.01\n",
      "800 800\n",
      "End of epoch 17: err(train)=1.39 acc(train)=0.51 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 18: err(train)=1.38 acc(train)=0.52 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 19: err(train)=1.37 acc(train)=0.52 time(train)=13.28\n",
      "800 800\n",
      "End of epoch 20: err(train)=1.36 acc(train)=0.53 time(train)=13.00\n",
      "                 err(valid)=1.46 acc(valid)=0.49\n",
      "800 800\n",
      "End of epoch 21: err(train)=1.34 acc(train)=0.53 time(train)=13.02\n",
      "800 800\n",
      "End of epoch 22: err(train)=1.33 acc(train)=0.54 time(train)=13.00\n",
      "800 800\n",
      "End of epoch 23: err(train)=1.32 acc(train)=0.54 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 24: err(train)=1.31 acc(train)=0.54 time(train)=13.09\n",
      "800 800\n",
      "End of epoch 25: err(train)=1.30 acc(train)=0.55 time(train)=13.00\n",
      "                 err(valid)=1.44 acc(valid)=0.50\n",
      "800 800\n",
      "End of epoch 26: err(train)=1.29 acc(train)=0.55 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 27: err(train)=1.28 acc(train)=0.56 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 28: err(train)=1.27 acc(train)=0.56 time(train)=13.01\n",
      "800 800\n",
      "End of epoch 29: err(train)=1.26 acc(train)=0.56 time(train)=13.05\n",
      "800 800\n",
      "End of epoch 30: err(train)=1.25 acc(train)=0.57 time(train)=13.10\n",
      "                 err(valid)=1.42 acc(valid)=0.51\n",
      "800 800\n",
      "End of epoch 31: err(train)=1.24 acc(train)=0.57 time(train)=13.01\n",
      "800 800\n",
      "End of epoch 32: err(train)=1.23 acc(train)=0.58 time(train)=13.00\n",
      "800 800\n",
      "End of epoch 33: err(train)=1.22 acc(train)=0.58 time(train)=12.97\n",
      "800 800\n",
      "End of epoch 34: err(train)=1.21 acc(train)=0.58 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 35: err(train)=1.20 acc(train)=0.59 time(train)=13.77\n",
      "                 err(valid)=1.39 acc(valid)=0.52\n",
      "800 800\n",
      "End of epoch 36: err(train)=1.19 acc(train)=0.59 time(train)=12.97\n",
      "800 800\n",
      "End of epoch 37: err(train)=1.18 acc(train)=0.59 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 38: err(train)=1.17 acc(train)=0.59 time(train)=13.07\n",
      "800 800\n",
      "End of epoch 39: err(train)=1.16 acc(train)=0.60 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 40: err(train)=1.16 acc(train)=0.60 time(train)=12.97\n",
      "                 err(valid)=1.39 acc(valid)=0.51\n",
      "800 800\n",
      "End of epoch 41: err(train)=1.15 acc(train)=0.60 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 42: err(train)=1.14 acc(train)=0.61 time(train)=13.07\n",
      "800 800\n",
      "End of epoch 43: err(train)=1.13 acc(train)=0.61 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 44: err(train)=1.12 acc(train)=0.61 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 45: err(train)=1.12 acc(train)=0.61 time(train)=12.98\n",
      "                 err(valid)=1.41 acc(valid)=0.51\n",
      "800 800\n",
      "End of epoch 46: err(train)=1.11 acc(train)=0.62 time(train)=12.97\n",
      "800 800\n",
      "End of epoch 47: err(train)=1.10 acc(train)=0.62 time(train)=12.96\n",
      "800 800\n",
      "End of epoch 48: err(train)=1.09 acc(train)=0.62 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 49: err(train)=1.08 acc(train)=0.63 time(train)=13.22\n",
      "800 800\n",
      "End of epoch 50: err(train)=1.07 acc(train)=0.63 time(train)=12.99\n",
      "                 err(valid)=1.37 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 51: err(train)=1.07 acc(train)=0.64 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 52: err(train)=1.06 acc(train)=0.64 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 53: err(train)=1.05 acc(train)=0.64 time(train)=12.96\n",
      "800 800\n",
      "End of epoch 54: err(train)=1.05 acc(train)=0.64 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 55: err(train)=1.04 acc(train)=0.65 time(train)=12.97\n",
      "                 err(valid)=1.36 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 56: err(train)=1.03 acc(train)=0.65 time(train)=13.21\n",
      "800 800\n",
      "End of epoch 57: err(train)=1.02 acc(train)=0.65 time(train)=13.00\n",
      "800 800\n",
      "End of epoch 58: err(train)=1.02 acc(train)=0.65 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 59: err(train)=1.01 acc(train)=0.66 time(train)=12.94\n",
      "800 800\n",
      "End of epoch 60: err(train)=1.00 acc(train)=0.66 time(train)=12.95\n",
      "                 err(valid)=1.37 acc(valid)=0.52\n",
      "800 800\n",
      "End of epoch 61: err(train)=0.99 acc(train)=0.66 time(train)=13.51\n",
      "800 800\n",
      "End of epoch 62: err(train)=0.99 acc(train)=0.66 time(train)=13.47\n",
      "800 800\n",
      "End of epoch 63: err(train)=0.98 acc(train)=0.66 time(train)=12.94\n",
      "800 800\n",
      "End of epoch 64: err(train)=0.97 acc(train)=0.67 time(train)=12.96\n",
      "800 800\n",
      "End of epoch 65: err(train)=0.97 acc(train)=0.67 time(train)=12.95\n",
      "                 err(valid)=1.37 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 66: err(train)=0.96 acc(train)=0.67 time(train)=12.93\n",
      "800 800\n",
      "End of epoch 67: err(train)=0.95 acc(train)=0.68 time(train)=12.94\n",
      "800 800\n",
      "End of epoch 68: err(train)=0.95 acc(train)=0.68 time(train)=12.95\n",
      "800 800\n",
      "End of epoch 69: err(train)=0.94 acc(train)=0.68 time(train)=12.95\n",
      "800 800\n",
      "End of epoch 70: err(train)=0.93 acc(train)=0.68 time(train)=12.99\n",
      "                 err(valid)=1.37 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 71: err(train)=0.92 acc(train)=0.69 time(train)=12.94\n",
      "800 800\n",
      "End of epoch 72: err(train)=0.92 acc(train)=0.69 time(train)=12.95\n",
      "800 800\n",
      "End of epoch 73: err(train)=0.91 acc(train)=0.69 time(train)=12.93\n",
      "800 800\n",
      "End of epoch 74: err(train)=0.91 acc(train)=0.69 time(train)=12.95\n",
      "800 800\n",
      "End of epoch 75: err(train)=0.90 acc(train)=0.70 time(train)=12.97\n",
      "                 err(valid)=1.37 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 76: err(train)=0.89 acc(train)=0.70 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 77: err(train)=0.89 acc(train)=0.70 time(train)=12.95\n",
      "800 800\n",
      "End of epoch 78: err(train)=0.88 acc(train)=0.70 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 79: err(train)=0.88 acc(train)=0.71 time(train)=12.98\n",
      "800 800\n",
      "End of epoch 80: err(train)=0.87 acc(train)=0.71 time(train)=13.00\n",
      "                 err(valid)=1.38 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 81: err(train)=0.86 acc(train)=0.71 time(train)=12.96\n",
      "800 800\n",
      "End of epoch 82: err(train)=0.85 acc(train)=0.71 time(train)=12.96\n",
      "800 800\n",
      "End of epoch 83: err(train)=0.85 acc(train)=0.72 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 84: err(train)=0.84 acc(train)=0.72 time(train)=13.00\n",
      "800 800\n",
      "End of epoch 85: err(train)=0.84 acc(train)=0.72 time(train)=13.00\n",
      "                 err(valid)=1.38 acc(valid)=0.54\n",
      "800 800\n",
      "End of epoch 86: err(train)=0.83 acc(train)=0.72 time(train)=12.99\n",
      "800 800\n",
      "End of epoch 87: err(train)=0.82 acc(train)=0.73 time(train)=13.03\n",
      "800 800\n",
      "End of epoch 88: err(train)=0.82 acc(train)=0.73 time(train)=13.02\n",
      "800 800\n",
      "End of epoch 89: err(train)=0.81 acc(train)=0.73 time(train)=13.03\n",
      "800 800\n",
      "End of epoch 90: err(train)=0.81 acc(train)=0.73 time(train)=13.03\n",
      "                 err(valid)=1.39 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 91: err(train)=0.80 acc(train)=0.74 time(train)=13.04\n",
      "800 800\n",
      "End of epoch 92: err(train)=0.79 acc(train)=0.74 time(train)=13.03\n",
      "800 800\n",
      "End of epoch 93: err(train)=0.78 acc(train)=0.74 time(train)=13.05\n",
      "800 800\n",
      "End of epoch 94: err(train)=0.78 acc(train)=0.74 time(train)=13.05\n",
      "800 800\n",
      "End of epoch 95: err(train)=0.77 acc(train)=0.74 time(train)=13.04\n",
      "                 err(valid)=1.41 acc(valid)=0.53\n",
      "800 800\n",
      "End of epoch 96: err(train)=0.77 acc(train)=0.75 time(train)=13.06\n",
      "800 800\n",
      "End of epoch 97: err(train)=0.76 acc(train)=0.75 time(train)=13.03\n",
      "800 800\n",
      "End of epoch 98: err(train)=0.76 acc(train)=0.75 time(train)=13.06\n",
      "800 800\n",
      "End of epoch 99: err(train)=0.75 acc(train)=0.75 time(train)=13.03\n",
      "800 800\n",
      "End of epoch 100: err(train)=0.74 acc(train)=0.75 time(train)=13.07\n",
      "                 err(valid)=1.43 acc(valid)=0.52\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "train_err = []\n",
    "valid_acc = []\n",
    "valid_err = []\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "with tf.Session() as sess:\n",
    "    run_start_time = time.time()\n",
    "    sess.run(init)\n",
    "    for e in range(100):\n",
    "        running_error = 0.\n",
    "        running_accuracy = 0.\n",
    "        start_time = time.time()\n",
    "        count = 0\n",
    "        for input_batch, target_batch in train_data:\n",
    "            count += 1\n",
    "            _, batch_error, batch_acc = sess.run(\n",
    "                [train_step, error, accuracy], \n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "            running_error += batch_error\n",
    "            running_accuracy += batch_acc\n",
    "        print count, train_data.num_batches\n",
    "        epoch_time = time.time() - start_time\n",
    "        running_error /= train_data.num_batches\n",
    "        running_accuracy /= train_data.num_batches \n",
    "        train_acc.append(running_accuracy)\n",
    "        train_err.append(running_error)\n",
    "        print('End of epoch {0:02d}: err(train)={1:.2f} acc(train)={2:.2f} time(train)={3:.2f}'\n",
    "              .format(e + 1, running_error, running_accuracy, epoch_time))\n",
    "        if (e + 1) % 5 == 0:\n",
    "            valid_error = 0.\n",
    "            valid_accuracy = 0.\n",
    "            for input_batch, target_batch in valid_data:\n",
    "                batch_error, batch_acc = sess.run(\n",
    "                    [error, accuracy], \n",
    "                    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "                valid_error += batch_error\n",
    "                valid_accuracy += batch_acc\n",
    "            valid_error /= valid_data.num_batches\n",
    "            valid_accuracy /= valid_data.num_batches\n",
    "            valid_acc.append(valid_accuracy)\n",
    "            valid_err.append(valid_error)\n",
    "            print('                 err(valid)={0:.2f} acc(valid)={1:.2f}'\n",
    "                   .format(valid_error, valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_seed = 12345678\n",
    "rng = np.random.RandomState(random_seed)\n",
    "train_data = CIFAR100DataProvider('train', batch_size=50, rng=rng)\n",
    "valid_data = CIFAR100DataProvider('valid', batch_size=50, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1]], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')\n",
    "num_hidden = 1000\n",
    "num_bottleneck_layer = 100\n",
    "\n",
    "with tf.name_scope('fc-layer-1'):\n",
    "    hidden_1 = fully_connected_layer(inputs, train_data.inputs.shape[1], num_hidden)\n",
    "with tf.name_scope('fc-layer-2'):\n",
    "    hidden_2= fully_connected_layer(hidden_1, num_hidden, num_hidden)\n",
    "with tf.name_scope('fc-layer-3'):\n",
    "    hidden_3 = fully_connected_layer(hidden_2, num_hidden, num_hidden)\n",
    "with tf.name_scope('fc-layer-4'):\n",
    "    hidden_4 = fully_connected_layer(hidden_3, num_hidden, num_bottleneck_layer)\n",
    "with tf.name_scope('output-layer'):\n",
    "    outputs = fully_connected_layer(hidden_4, num_bottleneck_layer, train_data.num_classes, tf.identity)\n",
    "with tf.name_scope('error'):\n",
    "    error = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(outputs, targets))\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1)), \n",
    "            tf.float32))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(error)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 01: err(train)=4.37 acc(train)=0.05\n",
      "End of epoch 02: err(train)=3.94 acc(train)=0.11\n",
      "End of epoch 03: err(train)=3.72 acc(train)=0.14\n",
      "End of epoch 04: err(train)=3.58 acc(train)=0.17\n",
      "End of epoch 05: err(train)=3.45 acc(train)=0.19\n",
      "                 err(valid)=3.52 acc(valid)=0.18\n",
      "End of epoch 06: err(train)=3.35 acc(train)=0.21\n",
      "End of epoch 07: err(train)=3.24 acc(train)=0.23\n",
      "End of epoch 08: err(train)=3.15 acc(train)=0.24\n",
      "End of epoch 09: err(train)=3.06 acc(train)=0.26\n",
      "End of epoch 10: err(train)=2.97 acc(train)=0.28\n",
      "                 err(valid)=3.35 acc(valid)=0.21\n",
      "End of epoch 11: err(train)=2.88 acc(train)=0.30\n",
      "End of epoch 12: err(train)=2.79 acc(train)=0.31\n",
      "End of epoch 13: err(train)=2.69 acc(train)=0.33\n",
      "End of epoch 14: err(train)=2.60 acc(train)=0.35\n",
      "End of epoch 15: err(train)=2.51 acc(train)=0.37\n",
      "                 err(valid)=3.29 acc(valid)=0.24\n",
      "End of epoch 16: err(train)=2.40 acc(train)=0.40\n",
      "End of epoch 17: err(train)=2.32 acc(train)=0.41\n",
      "End of epoch 18: err(train)=2.22 acc(train)=0.44\n",
      "End of epoch 19: err(train)=2.12 acc(train)=0.46\n",
      "End of epoch 20: err(train)=2.02 acc(train)=0.48\n",
      "                 err(valid)=3.36 acc(valid)=0.25\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "train_data.reset()\n",
    "valid_data.reset()\n",
    "for e in range(20):\n",
    "    running_error = 0.\n",
    "    running_accuracy = 0.\n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [train_step, error, accuracy], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_accuracy += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_accuracy /= train_data.num_batches\n",
    "    print('End of epoch {0:02d}: err(train)={1:.2f} acc(train)={2:.2f}'\n",
    "          .format(e + 1, running_error, running_accuracy))\n",
    "    if (e + 1) % 5 == 0:\n",
    "        valid_error = 0.\n",
    "        valid_accuracy = 0.\n",
    "        for input_batch, target_batch in valid_data:\n",
    "            batch_error, batch_acc = sess.run(\n",
    "                [error, accuracy], \n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "            valid_error += batch_error\n",
    "            valid_accuracy += batch_acc\n",
    "        valid_error /= valid_data.num_batches\n",
    "        valid_accuracy /= valid_data.num_batches\n",
    "        print('                 err(valid)={0:.2f} acc(valid)={1:.2f}'\n",
    "               .format(valid_error, valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting test data classes and creating a Kaggle submission file\n",
    "\n",
    "An optional [Kaggle in Class](https://inclass.kaggle.com/c/ml2016-7-cifar-100) competition (see email for invite link, you will need to sign-up with a `ed.ac.uk` email address to be able to enter) is being run on the CIFAR-100 (fine-grained) classification task. The scores for the competition are calculated by calculating the proportion of classes correctly predicted on the test set inputs (for which no class labels are provided). Half of the 10000 test inputs are used to calculate a public leaderboard score which will be visible while the competition is in progress and the other half are used to compute the private leaderboard score which will only be unveiled at the end of the competition. Each entrant can make up to two submissions of predictions each day during the competition.\n",
    "\n",
    "The code and helper function below illustrate how to use the predicted outputs of the TensorFlow network model we just trained to create a submission file which can be uploaded to Kaggle. The required format of the submission file is a `.csv` (Comma Separated Variable) file with two columns: the first is the integer index of the test input in the array in the provided data file (i.e. first row 0, second row 1 and so on) and the second column the corresponding predicted class label as an integer between 0 and 99 inclusive. The predictions must be preceded by a header line as in the following example\n",
    "\n",
    "```\n",
    "Id,Class\n",
    "0,81\n",
    "1,35\n",
    "2,12\n",
    "...\n",
    "```\n",
    "\n",
    "Integer class label predictions can be computed from the class probability outputs of the model by performing an `argmax` operation along the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_inputs = np.load(os.path.join(os.environ['MLP_DATA_DIR'], 'cifar-100-test-inputs.npz'))['inputs']\n",
    "test_predictions = sess.run(tf.nn.softmax(outputs), feed_dict={inputs: test_inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_kaggle_submission_file(predictions, output_file, overwrite=False):\n",
    "    if predictions.shape != (10000, 100):\n",
    "        raise ValueError('predictions should be an array of shape (10000, 25).')\n",
    "    if not (np.all(predictions >= 0.) and \n",
    "            np.all(predictions <= 1.)):\n",
    "        raise ValueError('predictions should be an array of probabilities in [0, 1].')\n",
    "    if not np.allclose(predictions.sum(-1), 1):\n",
    "        raise ValueError('predictions rows should sum to one.')\n",
    "    if os.path.exists(output_file) and not overwrite:\n",
    "        raise ValueError('File already exists at {0}'.format(output_file))\n",
    "    pred_classes = predictions.argmax(-1)\n",
    "    ids = np.arange(pred_classes.shape[0])\n",
    "    np.savetxt(output_file, np.column_stack([ids, pred_classes]), fmt='%d',\n",
    "               delimiter=',', header='Id,Class', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_kaggle_submission_file(test_predictions, 'cifar-100-example-network-submission.csv', True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
