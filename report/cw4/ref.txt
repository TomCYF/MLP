CIFAR-10 dataset characters
Subjects may vary in size by an order of magnitude (i.e., some images show only the head of a bird, others an entire bird from a distance). Colors and textures of objects/animals also vary greatly.

Augumentation methods [Multi-column deep neural networks for image classification]
augmenting the training set with randomly(by at most 5%) translated images greatly decreases the error from 28% to 20%(the NN-inherent local translation invariance by itself is not sufficient). But additional scaling(up to +- 15%), totation(up to +- 5du), and up to 15% translation, the individual net errors decrease by another 3%.

Another aspect: DNN robustness.

https://www.quora.com/In-convolutional-neural-networks-what-effect-does-the-size-e-g-3x3-5x5-7x7-of-the-convolution-kernel-have-on-the-architecture-of-the-convolutional-neural-networks

# kernel size of CNN
Convolutional neural networks work on 2 assumptions -

    Low level features are local
    What's useful in one place will also be useful in other places

Kernel size should be determined by how strongly we believe in those assumptions for the problem at hand.

In one extreme case where we have 1x1 kernels, we are essentially saying low level features are per-pixel, and they don't affect neighbouring pixels at all, and that we should apply the same operation to all pixels.

In the other extreme, we have kernels the size of the entire image. In this case the CNN essentially becomes fully connected, and stops being a CNN, and we are no longer making any assumption on low level feature locality.

In practice, this is often done by just trying a few kernel sizes, and see what works best.
----------------------------------------------------------------------------

So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.

It turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: That if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a depth slice (e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias. With this parameter sharing scheme, the first Conv Layer in our example would now have only 96 unique set of weights (one for each depth slice), for a total of 96*11*11*3 = 34,848 unique weights, or 34,944 parameters (+96 biases). Alternatively, all 55*55 neurons in each depth slice will now be using the same parameters. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice.

Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input are faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a Locally-Connected Layer.

----------------------------------------------------------------------------

This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As we  discussed  in  the  introduction,  if  the  added  layers  can be constructed as identity mappings, a deeper model should have  training  error  no  greater  than  its  shallower  counter-part.   The  degradation  problem  suggests  that  the  solvers might have difficulties in approximating identity mappings by multiple nonlinear layers. With the residual learning re-formulation,  if identity mappings are optimal,  the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.
